{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import cv2\n",
    "import csv\n",
    "import PIL\n",
    "\n",
    "\n",
    "# Dataset_Path = '/content/drive/MyDrive/inaturalist_12K'\n",
    "train_path = os.path.join(Dataset_Path , 'train')\n",
    "test_path = os.path.join(Dataset_Path , 'val')\n",
    "# test_path1 = test_path\n",
    "import pathlib\n",
    "Dataset_Path = pathlib.Path(Dataset_Path)\n",
    "train_path = pathlib.Path(train_path)\n",
    "test_path = pathlib.Path(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "IMAGE_SIZE =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True, \n",
    "    seed=1234, \n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True, \n",
    "    seed=1234, \n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential(\n",
    "  [\n",
    "    experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(IMAGE_SIZE, \n",
    "                                                              IMAGE_SIZE,\n",
    "                                                              3)),\n",
    "    experimental.preprocessing.RandomRotation(0.2),\n",
    "    experimental.preprocessing.RandomZoom(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_conv_model(model_name):\n",
    "    if model_name == 'InceptionV3':\n",
    "        base_model = tf.keras.applications.inception_v3(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    if model_name == 'InceptionResNetV2':\n",
    "        base_model = tf.keras.applications.inception_resnet_v2(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    if model_name == 'ResNet50':\n",
    "        base_model = tf.keras.applications.resnet50(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    if model_name == 'Xception':\n",
    "        base_model = tf.keras.applications.xception(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    preprocess_input = base_model.preprocess_input\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    prediction_layer = tf.keras.layers.Dense(10)\n",
    "    inputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = preprocess_input(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    initial_epochs = 10\n",
    "    history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_ds)\n",
    "    return model,history\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
