{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import cv2\n",
    "import csv\n",
    "import PIL\n",
    "\n",
    "Dataset_Path = os.path.join(os.getcwd(),'inaturalist_12K')\n",
    "# Dataset_Path = '/content/drive/MyDrive/inaturalist_12K'\n",
    "train_path = os.path.join(Dataset_Path , 'train')\n",
    "test_path = os.path.join(Dataset_Path , 'val')\n",
    "# test_path1 = test_path\n",
    "import pathlib\n",
    "Dataset_Path = pathlib.Path(Dataset_Path)\n",
    "train_path = pathlib.Path(train_path)\n",
    "test_path = pathlib.Path(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5500)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9999 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True, \n",
    "    seed=1234, \n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9999 files belonging to 10 classes.\n",
      "Using 999 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True, \n",
    "    seed=1234, \n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_path, \n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Activation, MaxPooling2D, Dropout , BatchNormalization, experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential(\n",
    "  [\n",
    "    experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(IMAGE_SIZE, \n",
    "                                                              IMAGE_SIZE,\n",
    "                                                              3)),\n",
    "    experimental.preprocessing.RandomRotation(0.2),\n",
    "    experimental.preprocessing.RandomZoom(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pretrained_model(model_name, epochs=10):\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    prediction_layer = tf.keras.layers.Dense(10,activation='softmax')\n",
    "    inputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    if model_name == 'InceptionV3':\n",
    "        base_model = tf.keras.applications.InceptionV3(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), include_top=False, weights='imagenet')\n",
    "        x = tf.keras.applications.imagenet_utils.preprocess_input(x,mode='tf')\n",
    "    elif model_name == 'InceptionResNetV2':\n",
    "        base_model = tf.keras.applications.InceptionResNetV2(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "        x = tf.keras.applications.imagenet_utils.preprocess_input(x,mode='tf')\n",
    "    elif model_name == 'ResNet50':\n",
    "        base_model = tf.keras.applications.ResNet50(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "        x = tf.keras.applications.imagenet_utils.preprocess_input(x,mode='caffe')\n",
    "    elif model_name == 'Xception':\n",
    "        base_model = tf.keras.applications.Xception(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "        x = tf.keras.applications.imagenet_utils.preprocess_input(x,mode='tf')\n",
    "    else:\n",
    "        base_model = None\n",
    "        preprocess_input = None\n",
    "    base_model.trainable = False\n",
    "    x = base_model(x, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "    initial_epochs = epochs\n",
    "    history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=val_ds)\n",
    "    model.save('PartB_models' + os.sep + 'base_models' + os.sep + model_name)\n",
    "    model.save('PartB_models' + os.sep + 'models' + os.sep + model_name)\n",
    "#     return model,history,base_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in ['InceptionV3','InceptionResNetV2','ResNet50','Xception']:\n",
    "#     save_pretrained_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model(model_name, epochs=10):\n",
    "    base_model = tf.keras.models.load_model('PartB_models' + os.sep + 'base_models' + os.sep + model_name)\n",
    "    model = tf.keras.models.load_model('PartB_models' + os.sep + 'models' + os.sep + model_name)\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pretrain_conv_model('InceptionV3',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(sweep_id):\n",
    "    config_defaults = {\n",
    "        'epochs' : 10,\n",
    "        'model_name': 'InceptionV3',\n",
    "        'strategy': '1'\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    model,base_model = get_pretrained_model(config.model_name,config.epochs)\n",
    "    print(\"Number of layers in base Model: \", len(base_model.layers))\n",
    "    n = len(base_model.layers)\n",
    "    l = base_model.layers\n",
    "    fine_tune_at = n\n",
    "    base_model.trainable = True\n",
    "    if config.strategy == 0:\n",
    "        fine_tune_at = n\n",
    "    if config.strategy == 1:\n",
    "        fine_tune_at = n-1\n",
    "    if config.strategy == 2:\n",
    "        fine_tune_at = n-20\n",
    "    if config.strategy == 3:\n",
    "        fine_tune_at = n-50\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable =  False   \n",
    "    print(len(model.trainable_variables))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.00001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    fine_tune_epochs = config.epochs\n",
    "    total_epochs = epochs + fine_tune_epochs\n",
    "    history_fine = model.fit(train_ds,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=val_ds,\n",
    "                            callbacks=[WandbCallback()])\n",
    "    model.save('models'+os.sep+str(sweep_id)+os.sep+wandb.run.name)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mooizz (use `wandb login --relogin` to force relogin)\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Jaitesh/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"866040d7d81f67025d43e7d50ecd83d54b6cf977\", relogin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'model_name': {  \n",
    "            'values': ['InceptionV3','InceptionResNetV2','ResNet50','Xception']\n",
    "        },\n",
    "        'strategy':{\n",
    "            'values': [1,2,3]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kulg4dit\n",
      "Sweep URL: https://wandb.ai/mooizz/conv_inaturalist/sweeps/kulg4dit\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity=\"mooizz\",project=\"conv_inaturalist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dashing-brook-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mooizz/ConvolutionalNetwork\" target=\"_blank\">https://wandb.ai/mooizz/ConvolutionalNetwork</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mooizz/ConvolutionalNetwork/runs/2od6iiz0\" target=\"_blank\">https://wandb.ai/mooizz/ConvolutionalNetwork/runs/2od6iiz0</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\ConvolutionalNetwork\\wandb\\run-20210413_165426-2od6iiz0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in base Model:  8\n",
      "190\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b5ccc7f7ea9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinetune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-8dcda4bbe04f>\u001b[0m in \u001b[0;36mfinetune\u001b[1;34m(sweep_id)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mfine_tune_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mtotal_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfine_tune_epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     history_fine = model.fit(train_ds,\n\u001b[0;32m     33\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, finetune(sweep_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
